# 多字段特性及Mapping中配置自定义Analyzer

## 多字段类型

> 当我们为一个索引设置mapping的时候，我们可以给一个字段添加**子字段**，比如text类型的字段添加一个keword子字段，这样可以实现精确匹配，还可以对**子字段**设置对应的分词器

+ 厂商名字实现精确匹配
  + 增加一个keyword字段
+ 使用不同的analyzer
  + 不用语言
  + pinyin字段的搜索
  + 还支持为搜索和索引指定不通的analyzer

![image-20220510154730853](/Users/litian/Documents/lian2077/documents/documents/Elastic-Search/Elastic核心技术与实战/images/image-20220510154730853.png)

## Exact Values v.s Full Text

> 精确值和全文本

+ Exact Values 包括数字/ 日期/ 具体一个字符串（例如：”Apple Store“）

  + Elasticsearch中的keyword

+ 全文本，非结构化的文本数据

  + Elasticsearch中的text


![image-20220510154819265](/Users/litian/Documents/lian2077/documents/documents/Elastic-Search/Elastic核心技术与实战/images/image-20220510154819265.png)

## Exact Values不需要被分词

+ Elasticsearch为每个字段创建一个倒排索引
+ Exact Values在索引时，不许做特殊的分词处理

![image-20220510154912016](/Users/litian/Documents/lian2077/documents/documents/Elastic-Search/Elastic核心技术与实战/images/image-20220510154912016.png)

## 自定义分词

+ 当Elasticsearch自带的分词器无法满足时，可以自定义分词器。通过组合不同的组件实现
  + Character Filter
  + Tokenizer
  + Token Filter

### Character Filters

+ 在Tokenizer之前对文本进行处理，例如增加删除及替换字符。可以配置多个Character Filters。会影响Tokenizer的position 和 offset 信息
+ 一些自带的Character Filters
  + HTML strip - 去除html标签
  + Mapping - 字符串替换
  + Pattern replace - 正则匹配替换

### Tokenizer

+ 将原始的文本按照一定的规则，切分为词（term or token）
+ Elasticsearch 内置的Tokenizers
  + whitespace（以空格的方式分词） / standard / uax_url_email（以url和email的方式分词，通常输入的email会由@分隔分词，使用这种就不会了） / pattern （自定义的正则表达式来分词）  / keyword （不做任何处理，把输入的字符串当做一个term输出）/ path hierarchy（文件路径合适的分词）
+ 可以用Java开发插件，实现自己的 Tokenizer

### Token Filters

+ 将Tokenizer输出的单词（term），进行增加，修改，删除
+ 自带的Token Filters
  + Lowercase（转小写） / stop （去除一些停用词，比如  a  an  and ） /synonym（添加近义词）

## 设置一个Custom Analyzer

![image-20220510155617591](/Users/litian/Documents/lian2077/documents/documents/Elastic-Search/Elastic核心技术与实战/images/image-20220510155617591.png)

## 例子演示

### 分词演示

````json
# 分词演示
# character - 去除html标签
POST _analyze 
{
  "tokenizer": "keyword",
  "char_filter": ["html_strip"],
  "text": "<b>Hello world</b>"
}
````

```json
{
  "tokens" : [
    {
      "token" : "Hello world",
      "start_offset" : 3,
      "end_offset" : 18,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

### 使用character filter进行替换

````json
# 使用character filter进行替换
POST _analyze
{
  # 标准分词
  "tokenizer": "standard",
  "char_filter": [
    {
      "type": "mapping",
      "mappings": ["- => _"]
    }
  ],
  "text": "123-456, I-test! test-990 650-555-1234"
}
````

````json
{
  "tokens" : [
    {
      "token" : "123_456",
      "start_offset" : 0,
      "end_offset" : 7,
      "type" : "<NUM>",
      "position" : 0
    },
    {
      "token" : "I_test",
      "start_offset" : 9,
      "end_offset" : 15,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "test_990",
      "start_offset" : 17,
      "end_offset" : 25,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "650_555_1234",
      "start_offset" : 26,
      "end_offset" : 38,
      "type" : "<NUM>",
      "position" : 3
    }
  ]
}
````

````json
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [
      {
        "type": "mapping",
        "mappings": [":) => happy", ":( => sad"]
      }
    ],
    "text": ["I am felling :)", "felling :( yesterday"]
}
````

````json
{
  "tokens" : [
    {
      "token" : "I",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "am",
      "start_offset" : 2,
      "end_offset" : 4,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "felling",
      "start_offset" : 5,
      "end_offset" : 12,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "happy",
      "start_offset" : 13,
      "end_offset" : 15,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "felling",
      "start_offset" : 16,
      "end_offset" : 23,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "sad",
      "start_offset" : 24,
      "end_offset" : 26,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "yesterday",
      "start_offset" : 27,
      "end_offset" : 36,
      "type" : "<ALPHANUM>",
      "position" : 6
    }
  ]
}
````

### 正则表达式

````shell
GET _analyze
{
  "tokenizer": "standard",
  "char_filter": [
    {
      "type": "pattern_replace",
      "pattern": "http://(.*)",
      "replacement": "$1"
    }
  ],
  "text": ["http://www.elastic.co"]
}
````

````json
{
  "tokens" : [
    {
      "token" : "www.elastic.co",
      "start_offset" : 0,
      "end_offset" : 21,
      "type" : "<ALPHANUM>",
      "position" : 0
    }
  ]
}
````

### tokenizer演示

> 按目录进行分词

````json
POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": "/usr/local/a/b/c"
}
````

```json
{
  "tokens" : [
    {
      "token" : "/usr",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/usr/local",
      "start_offset" : 0,
      "end_offset" : 10,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/usr/local/a",
      "start_offset" : 0,
      "end_offset" : 12,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/usr/local/a/b",
      "start_offset" : 0,
      "end_offset" : 14,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/usr/local/a/b/c",
      "start_offset" : 0,
      "end_offset" : 16,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

### TokenFilters

#### whitespace与stop

> whitespace与stop会通过空格进行分词，但是一般的语气助词会被删除，比如the/on/in
>
> 以下大写的The没有去掉是因为是大写的，可以通过lowercase进行去除

````
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["stop"], 
  "text": ["The rain in Spain falls mainly on the plain."]
}   
````

````json
{
  "tokens" : [
    {
      "token" : "The",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "rain",
      "start_offset" : 4,
      "end_offset" : 8,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "Spain",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "falls",
      "start_offset" : 18,
      "end_offset" : 23,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "mainly",
      "start_offset" : 24,
      "end_offset" : 30,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "plain.",
      "start_offset" : 38,
      "end_offset" : 44,
      "type" : "word",
      "position" : 8
    }
  ]
}
````

````
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["lowercase","stop"], 
  "text": ["The girls in China are playing this game!"]
}
````

````json
{
  "tokens" : [
    {
      "token" : "girls",
      "start_offset" : 4,
      "end_offset" : 9,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "china",
      "start_offset" : 13,
      "end_offset" : 18,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "playing",
      "start_offset" : 23,
      "end_offset" : 30,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "game!",
      "start_offset" : 36,
      "end_offset" : 41,
      "type" : "word",
      "position" : 7
    }
  ]
}
````

### 自定义分词器

> 创建自己的索引，并设置mapping，自定义自己的分词器

````shell
PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "char_filter": [
            "emoticons"
          ],
          "tokenizer": "punctuation",
          "filter": [
            "lowercase",
            "english_stop"
          ]
        }
      },
      "tokenizer": {
        "punctuation": {
          "type": "pattern",
          "pattern": "[.,!?]"
        }
      },
      "char_filter": {
        "emoticons": {
          "type": "mapping",
          "mappings": [
            ":) => _happy_",
            ":( => _sad_"
          ]
        }
      },
      "filter": {
        "english_stop": {
          "type": "stop",
          "stopwords": "_english_"
        }
      }
    }
  }
}
````

````shell
POST my_index/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text": ["I'm a :) person, and you?"]
}
````

````json
{
  "tokens" : [
    {
      "token" : "i'm a _happy_ person",
      "start_offset" : 0,
      "end_offset" : 15,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : " and you",
      "start_offset" : 16,
      "end_offset" : 24,
      "type" : "word",
      "position" : 1
    }
  ]
}
````

