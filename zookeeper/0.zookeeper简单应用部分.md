# 1.重新认识Zookeeper？

**官网定义：**

它是一个分布式服务框架，是Apache Hadoop的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命令服务、集群管理、分布式应用配置项的管理等。

**ZK是什么：**

+ Zookeeper是一个数据库

  Zookeeper里面是通过节点也就是`znode`构成的，可以对节点进行增删改和存储数据，这点和数据库十分相像，通常可以通过控制这些节点，来监听集群中的节点信息。

+ Zookeeper是一个拥有文件系统特点的数据库

  一个znode节点可以包含子znode，同时也可以包含数据，znode 即是文件夹又是文件的概念，所以在 ZooKeeper 这里面就不叫文件也不叫文件夹，叫znode，每个znode有唯一的路径标识， 既能存储数据，也能创建子znode。但是`znode只适合存储非常小的数据，不能超过1M`，最好都小于1K。我们可以看到操作节点都离不开`/`这个符号，这一点和文件系统相似，所以我们说它和文件系统一样。

  zookeeper的文件系统的特点：

  + zk的文件系统和Linux的文件系统目录结构一样，**从`/`开始**
  + zk的**访问路径只有绝对路径**，没有相对路径。
  + zk中没有文件和目录的概念，**只有znode节点**，Znode既有文件的功能，又有目录的功能

+ Zookeeper是一个解决了数据一致性问题的分布式数据库

  Zookeeper可以通过修改配置文件如下，以及增加myid（这个文件在`dataDir`目录下，这个文件里面就只有一个数据就是集群第几号的值，比如server.1就是1）来进行集群配置实现数据的一致性

  ````properties
  # 第一个端口是通讯端口，即A服务器与集群中的 Leader 服务器交换信息的端口
  # 第二端口是选举通讯端口，表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。
  server.1=localhost:2887:3887 
  server.2=localhost:2888:3888
  server.3=localhost:2889:3889
  #server.4=localhost:2890:3890:observer 观察者提供读的服务，不提供写
  ````

  Zookeeper实现了CAP理论中的CP，而数据一致性则是实现了**最终一致性**

  **数据一致性分析：**

  + 弱一致性：数据不一致也能提供服务
  + 强一致性：在数据没有同步到对应的节点上的时候，倘若有其他节点访问被同步的节点的时候会阻塞比如加锁，只有数据一致的时候才提供服务。
  + 最终一致性：在数据同步没有完成的时候可以让请求读取到同步之前的数据，过一段时间再读到同步过后的数据。

  **CAP理论：**

  + 一致性：上面已经说了一致性了。

  + 分区容错性：

    比如将User模块和订单模块分区部署，可以在不同的网段，不同的机房，不同的城市，加入它们之间没有网络的话对用户来说就不能下单了，现在容错性就比较低。所以我们要保证网络稳定，才能保证分区容错性。

  + 可用性：比如User和订单服务之间的网络断开了，但是还是可以对外提供服务。

    

  **zk是怎么解决一致性的？**

  + **日常生活中是怎么解决一致性的？**

    比如领导说：今天下午3点开会，做一下未来规划，收到请回复

    看到通知的同事回复1

    而领导他会看有多少人回复了他这条通知，如果回复的人太少，他可能会再次进行通知，如果回复的人比较多了，他就放心了，他并不会统计是不是所有人都回复了

    等到下午3点，所有同事一起做未来的规划

    ![1568864464855](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1568864464855.png)

    **分析：**

    + 要素：

      1. 需要一个领导

         **领导者选举机制**

         + 普通人类选举

           1. 先投给自己(心里面投给自己)
           2. 和其他人沟通
           3. 选择一个比较“厉害”的人
           4. 投票箱
           5. 获得票数最多的人被选为leader

         + **zk领导者选举**

           + zk怎么选举一个比较厉害的节点，即数据最新的节点，通常请求分为如下：

             1. 事务性请求：create/set/delete

                每一个事务请求会有一个事务id，这个id会慢慢自增，可以通过zxid来判断，如果zxid一样可以根据myid来进行判断。

             2. 非事务性请求：get/exist/ls

             

           + 投票箱：

             1. 每个zk里面都有一个数组，放着zk自己的选票(投给了谁[一般先投给自己]，还有zxid)，zk会把自己的选票发送给彼此，借此来判断你投给的和我投给的zk的强弱，所以会有zxid。假设zk1收到zk2的选票和自己的选票（投给了自己）对比，发现zk2的选票的服务器节点的事务id比自己大（如果一样就对比myid的大小），然后将自己选票修改为zk2的选票再发送给zk2，即选举zk2为leader，然后同步数据
             2. 假设此时zk3进来，因为zk2已经是leader，所以zk3只能为follower
             3. 如果zk2接收到事务请求将自己的zxid更新为10，其他两个还是9，但是还没有同步过去，如果此时集群挂了，重新进行选举
             4. 重新选举zk3为leader，zk2的zxid回滚，因为事务请求没有同步是一次失败的请求，是无效的。
             5. follower挂掉后leader发现已经没有过半的follower跟随自己了，就不能对外提供服务了，此时要进行领导者选举的目的就是让整个集群不能对外提供服务 。

         **zk选举的时候是不对外提供服务的**

      2. **过半机制**

         超过一半的人都回复1领导就可以认为会议可以开了

      3. 预提交，ack(过半机制)，提交（2PC，即**两阶段提交**）

         + 普通两阶段提交

           比如下午3点开会，我需要占用你们3点的时间，即占用资源，就是**预提交**，然后就会收到回复，即**ack**，最后提交

         + zk下保证数据一致性的是**两阶段提交+过半机制**

           后面会讲到**数据在磁盘中的表示**以及**数据在内存中的表示**。简单来说就是数据在磁盘上面会存有一份事务日志和快照。

           所以leader接受到事务请求时候会如下：

           1. 生成事务日志并持久化
           2. 向其他zk节点发送事务日志
           3. 接受其他zk节点事务日志持久化结果
           4. 异步发送commit请求到follower
           5. follower接受到请求更新内存
           6. 处理日志更新DataTree，即内存中的zk数据

           然后follower接受到leader的事务日志请求的时候：

           1. 接受事务日志
           2. 持久化事务日志
           3. 反馈事务日志持久化结果，即**ack**
           4. 接受leader的commit请求
           5. 处理日志更新DataTree

           倘若在发送事务日志，即同步的时候follower节点挂掉了怎么办，Zookeeper采用的是过半机制，假设3台节点，leader自己还有一个ack，所以有2个ack已经过半，可以反馈给客户端了

           **如果client发送事务请求的是follower的？**

           follower会有一个转发的操作，将请求转发给leader，然后继续执行上面的步骤

         + **数据在磁盘中的表示**

           假设我们现在在Zookeeper中有一个数据节点，节点名为`/datanode`，内容为`125`，该节点是持久化节点，所以该节点信息会保存在文件中。

           可能大家都会认为是类似下面这样方式保存在磁盘文件中的，方法一：

           | 节点名    | 节点内容 |
           | --------- | -------- |
           | /datanode | 125      |

           但是除开这种表示方法，还有另外一种表示方法，**快照+事务日志**，比如方法二：

           当前快照：

           | 节点名    | 节点内容 |
           | --------- | -------- |
           | /datanode | 120      |

           当前事务日志：

           | 事务ID  | 操作   | 节点名    | 节点内容修改前 | 节点内容修改后 |
           | ------- | ------ | --------- | -------------- | -------------- |
           | 1000010 | update | /datanode | 120            | 121            |
           | 1000011 | update | /datanode | 121            | 125            |

           乍一看方法二比方法一要更复杂，并且占用的磁盘更多。但是我们上文提到过，Zookeeper集群中的节点在处理事务性请求时，需要将**事务操作**同步给其他节点，所以这里的事务操作是一定要进行持久化的，以便在同步给其他节点时出现异常进行补偿。所以就出现了**事务日志**。实际上事务日志还运行数据进行**回滚**，这个在两阶段提交中也是非常重要的。

           那么**快照**又有什么用呢？事务日志一定要有，但是随着时间的推移，日志肯定会越来越多，所以肯定不能持久化历史上所有的日志，所以Zookeeper会定时的进行**快照**，并删除之前的日志。

           那么如果按方法二这么存储数据，在对数据进行查询时就不太方便了。上文说到，Zookeeper为了提高数据的查询速度，会在内存中也存储一份数据，那么内存中的这份数据又该怎么存呢？

         + **数据在内存中的表示**

           Zookeeper中的数据在内存中的表示其实和上文的方法一很类似，只是Zookeeper中的数据是具有文件目录特点的，说白了就是Zookeeper中的数据节点的名字一定要以`“/”`开头，这样就导致Zookeeper中的数据类似一颗树：

           ![](https://cdn.nlark.com/yuque/0/2019/png/365147/1561620207784-24f721dc-43fa-4074-928c-056339f10657.png)

           一颗具有父子层级的多叉树，在Zookeeper源码中叫**DataTree**。

         + **请求处理逻辑**

           请看下图：

           ![](https://cdn.nlark.com/yuque/0/2019/png/365147/1561621802788-05b75f79-a8cb-4c2a-b271-38910c25fd53.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_10,text_6bKB54-t5a2m6Zmi5ZGo55Gc,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10)

           > 请注意，对于上图，Zookeeper真正的底层实现，zk1是Leader，zk2和zk3是Learner，是根据[领导者选举](https://mp.weixin.qq.com/s/z73f6rQXYvh2byfkO8tMoA)选出来的。

         + **非事务性请求直接读取DataTree上的内容，DataTree是在内存中的，所以会非常快。**

      4. 同步

         比如某一个同事因为原因错过了开会，第二天来的时候需要和开了会的同事同步信息。

    + 

  + 1111

+ Zookeeper是一个具有发布和订阅功能的分布式数据库(watch)

# 2.**Zookeeper如何解决脑裂问题**

## 什么是脑裂

脑裂(split-brain)就是“大脑分裂”，也就是本来一个“大脑”被拆分了两个或多个“大脑”，我们都知道，如果一个人有多个大脑，并且相互独立的话，那么会导致人体“手舞足蹈”，“不听使唤”。



脑裂通常会出现在集群环境中，比如ElasticSearch、Zookeeper集群，而这些集群环境有一个统一的特点，就是它们有一个大脑，比如ElasticSearch集群中有Master节点，Zookeeper集群中有Leader节点。



本篇文章着重来给大家讲一下Zookeeper中的脑裂问题，以及是如果解决脑裂问题的。

## Zookeeper集群中的脑裂场景

对于一个集群，想要提高这个集群的可用性，通常会采用多机房部署，比如现在有一个由6台zkServer所组成的一个集群，部署在了两个机房：

![image.png](https://cdn.nlark.com/yuque/0/2019/png/365147/1563867147007-e5000b66-fbe7-4958-89c7-11800de04f7c.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_10,text_6bKB54-t5a2m6Zmi5ZGo55Gc,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10)



正常情况下，此集群只会有一个Leader，那么如果机房之间的网络断了之后，两个机房内的zkServer还是可以相互通信的，如果**不考虑过半机制**，那么就会出现每个机房内部都将选出一个Leader。

**![image.png](https://cdn.nlark.com/yuque/0/2019/png/365147/1563867309583-b3c9d494-d91e-41f0-bb1f-310354cc14c4.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_10,text_6bKB54-t5a2m6Zmi5ZGo55Gc,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10)**



这就相当于原本一个集群，被分成了两个集群，出现了两个“大脑”，这就是脑裂。



对于这种情况，我们也可以看出来，原本应该是统一的一个集群对外提供服务的，现在变成了两个集群同时对外提供服务，如果过了一会，断了的网络突然联通了，那么此时就会出现问题了，两个集群刚刚都对外提供服务了，数据该怎么合并，数据冲突怎么解决等等问题。



刚刚在说明脑裂场景时，有一个前提条件就是没有考虑过半机制，所以实际上Zookeeper集群中是不会出现脑裂问题的，而不会出现的原因就跟过半机制有关。

## 过半机制

在领导者选举的过程中，如果某台zkServer获得了超过半数的选票，则此zkServer就可以成为Leader了。

过半机制的源码实现其实非常简单：



````java
public class QuorumMaj implements QuorumVerifier {
    private static final Logger LOG = LoggerFactory.getLogger(QuorumMaj.class);
    
    int half;
    
    // n表示集群中zkServer的个数（准确的说是参与者的个数，参与者不包括观察者节点）
    public QuorumMaj(int n){
        this.half = n/2;
    }

    // 验证是否符合过半机制
    public boolean containsQuorum(Set<Long> set){
        // half是在构造方法里赋值的
        // set.size()表示某台zkServer获得的票数
        return (set.size() > half);
    }
    
}
````



大家仔细看一下上面方法中的注释，核心代码就是下面两行：



````java
this.half = n/2;
return (set.size() > half);
````



举个简单的例子：

如果现在集群中有5台zkServer，那么half=5/2=2，那么也就是说，领导者选举的过程中至少要有三台zkServer投了同一个zkServer，才会符合过半机制，才能选出来一个Leader。



那么有一个问题我们想一下，**选举的过程中为什么一定要有一个过半机制验证？**

因为这样不需要等待所有zkServer都投了同一个zkServer就可以选举出来一个Leader了，这样比较快，所以叫快速领导者选举算法呗。



那么再来想一个问题，**过半机制中为什么是大于，而不是大于等于呢？**



这就是更脑裂问题有关系了，比如回到上文出现脑裂问题的场景：

![image.png](https://cdn.nlark.com/yuque/0/2019/png/365147/1563868159921-23d50d01-ec38-45e3-bb93-76f4bb27f896.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_10,text_6bKB54-t5a2m6Zmi5ZGo55Gc,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10)



当机房中间的网络断掉之后，机房1内的三台服务器会进行领导者选举，但是此时过半机制的条件是set.size() > 3，也就是说至少要4台zkServer才能选出来一个Leader，所以对于机房1来说它不能选出一个Leader，同样机房2也不能选出一个Leader，这种情况下整个集群当机房间的网络断掉后，整个集群将没有Leader。



而如果过半机制的条件是set.size() >= 3，那么机房1和机房2都会选出一个Leader，这样就出现了脑裂。所以我们就知道了，为什么过半机制中是**大于**，而不是**大于等于**。就是为了防止脑裂。



如果假设我们现在只有5台机器，也部署在两个机房：

![image.png](https://cdn.nlark.com/yuque/0/2019/png/365147/1563865876119-268f52aa-3fce-4337-ab5a-ed0e19fb388c.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_10,text_6bKB54-t5a2m6Zmi5ZGo55Gc,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10)



此时过半机制的条件是set.size() > 2，也就是至少要3台服务器才能选出一个Leader，此时机房件的网络断开了，对于机房1来说是没有影响的，Leader依然还是Leader，对于机房2来说是选不出来Leader的，此时整个集群中只有一个Leader。



所以，我们可以总结得出，有了过半机制，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题。

## Observer

在Zookeeper里面角色分为观察者和非观察者即我们的leader节点和follower节点

**为什么提供observer节点？**

是为了提高我们读的性能，因为observer不参与领导者选举以及写的操作，只是为了提供读的操作，我们可以这样想，如果我们加入的follower的节点呢？虽然我们也能提高读的性能，但是随着领导者选举的机制，投票数和ack什么的都会增加，我们也能那么是不是会增加两阶段提交的时间？所以也会影响我们写的性能。**影响我们写的性能的使我们ack的一步而不是commit那一步**，为什么呢？因为commit只是一个循环放进队列，真正影响的是ack。

另外通过源码我们可以看到，在commit之后，也会把数据包发送一份给observer，所以observer只是参与了一点点的领导者选举。

# Zab协议

解决了分布式系统的数据一致性的协议。待写....

# Zookeeper领导者选举源码分析

首先我们来分析集群启动需要做哪些操作

1. 初始化配置文件
2. 监听端口
3. 加载数据到内存(DataTree)
4. 启动的时候**领导者选举**
5. 初始化服务器(角色)